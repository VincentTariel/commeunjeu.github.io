\documentclass{book}
\usepackage{commeunjeustyle}
\begin{document}

\chapter*{Lien entre espaces vectoriels, matrices et applications linéaires}

\begin{Texte}
Ce chapitre est l'aboutissement de toutes les notions d'algèbre linéaire vues jusqu'ici : espaces vectoriels, matrices et
applications linéaires. Nous allons voir que dans le cas des espaces vectoriels de dimension finie, l'étude des applications linéaires est \impo{équivalente} à l'étude des matrices. On choisira le plus approprié en fonction du problème : matriciel si des calculs doivent être effectués (par exemple pour des applications numériques en deep-learning ou en rendu graphique pour les jeux vidéos) ou applications linéaires rendant les démonstrations plus "élégantes" et plus générales car parfois vrai aussi en dimension infinie.\\
Dans ce chapitre, tous les espaces vectoriels sont de dimension finie.\\
\end{Texte}

\section{Lien entre les vecteurs et les matrices}
\subsection{Passage vecteurs$\leftrightarrow$matrices} 
\begin{Definition}[Matrice d'un vecteur]
Soit $\mathcal{B} = (\Vect{e_1},\dots,\Vect{e_p})$ une base de $E$.\\
On sait que tout vecteur $\Vect{x}\in  E$ se décompose de façon unique sous la forme
$\Vect{x} = x_1 \Vect{e_1} + \dots + x_p \Vect{e_p}$ où $x_1,\dots, x_p \in   \K$.\\
On appelle \defi{matrice du vecteur}  $\Vect{x}$ dans la base $\mathcal{B}$ la matrice colonne :
$$ \uMat{\Vect{x}}{\mathcal{B}} = \begin{pmatrix}
x_1\\\vdots\\x_p\\
\end{pmatrix}.$$
\end{Definition}

\begin{Exemple}[Polynôme] 
Déterminer la matrice du vecteur $P=1-X^2$ dans la base canonique $\mathcal{B}=(1,X,X^2,\dots,X^n)$ de $\R_n[X]$.
\begin{Correction}
On a $$\uMat{P}{\mathcal{B}}=\begin{pmatrix}1\\0\\-1\\0\\\vdots \\0\\\end{pmatrix}.$$
\end{Correction}
\end{Exemple}
\begin{Exemple}[Polynôme d'interpolation de Lagrange] 
Déterminer la matrice du vecteur $P=1-X^2$ dans la base canonique $\mathcal{B}=(1,X,X^2,\dots,X^n)$ de $\R_n[X]$.
\begin{Correction}
On a $$\uMat{P}{\mathcal{B}}=\begin{pmatrix}1\\0\\-1\\0\\\vdots \\0\\\end{pmatrix}.$$
\end{Correction}
\end{Exemple}
\begin{Exemple}[Polynômes d'interpolation de Lagrange]
Soit $(a_0, a_1,  \dots , a_n)$ des éléments distincts de $\K$ et $\mathcal{L} = (L_0, · · · , L_n)$ la base (de $\K_n[X]$) des polynômes d'interpolation de Lagrange attachée à ces valeurs, c'est-à-dire, $L_i(a_j)\begin{cases}1  &\text{si}i=j\\0  &\text{si} i\neq j\end{cases}.$\\
Déterminer $\uMat{P}{\mathcal{L}}$ avec $P\in \K_n[X]$.
\begin{Correction}
Comme$ P =  P(a_0)L_0 + \dots + P(a_n)Ln$, on a :
$$\uMat{P}{\mathcal{L}}=\begin{pmatrix}P(a_0)\\\vdots\\P(a_n)\\ \end{pmatrix}.$$
\end{Correction}
\end{Exemple}
\begin{Definition}[Matrice d'une famille de vecteurs]
Soit $\mathcal{B} = (\Vect{e_1},\dots,\Vect{e_p})$ une base de $E$.\\
Soit $\mathcal{F} = (\Vect{x_1},\dots,\Vect{x_n})$ une famille de vecteurs de $E$ tel que 
$\Vect{x_i} = a_{i1} \Vect{e_1} + \dots + a_{ip} \Vect{e_p}$ pour tout $i\in\Intf{1}{n}$.\\
On appelle \defi{matrice de la famille de vecteurs}  $\mathcal{F}$ dans la base $\mathcal{B}$ la matrice :
$$ \uMat{\mathcal{F}}{\mathcal{B}} = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1p}\\
a_{21} & a_{22} & \cdots & a_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{np}\\
\end{pmatrix}.$$
\end{Definition}

\begin{Exemple}[Polynôme] 
Déterminer la matrice de la famille  $\mathcal{F}=((X+1)^i)_{0\leq i\leq n}$ dans la base canonique de $\R_n[X]$.
\begin{Correction}
On a $P_i=(X+1)^i=\sum_{k=0}^i\begin{pmatrix}i\\k \end{pmatrix}X^k$  d'où
$$ \uMat{\mathcal{F}}{\mathcal{B}}=\begin{pmatrix}
\begin{pmatrix}0\\0 \end{pmatrix} & \begin{pmatrix}1\\0 \end{pmatrix} & \cdots & \begin{pmatrix}n\\0 \end{pmatrix}\\
0 & \begin{pmatrix}1\\1 \end{pmatrix} & \cdots & \begin{pmatrix}n\\1 \end{pmatrix}\\
\vdots &  & \ddots & \vdots\\
0 & \cdots  & 0 & \begin{pmatrix}n\\n \end{pmatrix}\\
\end{pmatrix}.$$
\end{Correction}
\end{Exemple}
\begin{Definition}[Famille de vecteurs colonnes d'une matrice]
Soit $A\in\MnpK$ avec $C_1,\dots, C_p$ ses colonnes.\\
On appelle \defi{famille des vecteurs colonnes} de $A$ la famille  $(C_1,\dots, C_p)$. 
\end{Definition}
\begin{Exemple}[Polynôme] 

Déterminer le famille des vecteurs colonnes de la matrice $A=\begin{pmatrix}1&0\\2&3\end{pmatrix}$
\begin{Correction}
La famille est $(\begin{pmatrix}1\\2\end{pmatrix},\begin{pmatrix}0\\3\end{pmatrix}).$
\end{Correction}
\end{Exemple}

\subsection{}
\begin{Proposition}[Colonnes d'un produit matriciel]%ref=matrices_colonnes_produit
Soit $A\in\MnpK$ avec $C_1,\dots, C_p$ ses colonnes et $\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}\in\K^p$.\\
Alors
$$A\times\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}=x_1C_1+\dots+x_pC_p$$
\end{Proposition}
\begin{Proposition}
Soit $A\in\MnK$.
$A$ est inversible si et seulement si $C_1,\dots, C_n$  est une base de $\K^n$.
\end{Proposition}
\begin{Demonstration}
\begin{itemize}
\item \impo{$\Rightarrow$} Supposons $A$ inversible.\\
\begin{itemize}
\item \impo{Génératrice} Soit $Y\in \K^n$.\\
Comme $A$ est inversible, on pose $X=A^{-1}Y=\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}$.\\
On a $x_1C_1+\dots+x_pC_p=A\times\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}=AX=A(A^{-1}Y)=Y$.
\item \impo{Libre} Comme $C_1,\dots, C_n$ est une famille génératrice de $n$ vecteurs dans un espace vectoriel de dimension $n$, $C_1,\dots, C_n$ es
   
\end{itemize}
\end{Demonstration}

\section{Lien entre les matrices et les applications linéaires}

\subsection{Définitions des matrices d'application linéaires} 
\begin{Definition}[Matrice d'une application linéaire]
Soit $\mathcal{B} = (\Vect{e_1},\dots,\Vect{e_p})$ une base de $E$ et $\mathcal{B}' = (\Vect{f_1},\dots,\Vect{f_n})$ une base de $F$.\\
Pour tout $j\in  \{1,\dots,p\}$, le vecteur $u(\Vect{e_j})$ se décompose dans la base $\mathcal{B}'$:
\[u(\Vect{e_j})=a_{1j}.\Vect{f_1}+\dots +a_{nj}.\Vect{f_n}. \]
On appelle \defi{matrice de l'application linéaire} $u$ dans les base $\mathcal{B}$ et  $\mathcal{B}'$ la matrice   :
\begin{center}
  \begin{tikzpicture}[
      every left delimiter/.style={xshift=0.75em},
      every right delimiter/.style={xshift=-0.75em},
      dots/.style={
        line width=1pt,
        line cap=round,
        dash pattern=on 0pt off 5pt,
        shorten >=.1cm,
    shorten <=.1cm}]
    \matrix (M) [
      matrix of nodes,
      left delimiter=(,
      right delimiter=),
    ]{
      \node (A) {$a_{11}$}; &[1.1cm] \node (B) {$a_{1p}$}; \\[1.1cm]
      \node (C) {$a_{n1}$}; &        \node (D) {$a_{np}$}; \\
    };

    \draw (M.west) node[left] {$\uMat{u}{\mathcal{B}}{\mathcal{B}'}=\uMat{u(\mathcal{B})}{\mathcal{B}'}=$};
    \draw [dots] (A.east)  -- (B.west);
    \draw [dots] (C.east)  -- (D.west);
    \draw [dots] (A.south) -- (C.north);
    \draw [dots] (B.south) -- (D.north);
    \draw (A) [yshift=0.7cm] node (E) {$u(\Vect{e_1})$};
    \draw (B) [yshift=0.7cm] node (F) {$u(\Vect{e_p})$};
    \draw (B) [xshift=1cm]   node (G) {$\Vect{f_1}$};
    \draw (D) [xshift=1cm]   node (H) {$\Vect{f_n}$};
    \draw [dots] (E.east)  -- (F.west);
    \draw [dots] (G.south) -- (H.north);
  \end{tikzpicture}.
\end{center}
Les autres notations sont $ [u]_\mathcal{B}^{\mathcal{B}'}= \mat(u,\mathcal{B} \to \mathcal{B}') = \mat_{\mathcal{B}\to\mathcal{B}'}(u).$\\
Dans le cas d'un endomorphisme, $E=F$  et $\mathcal{B}=\mathcal{B}'$, on la note $[u]_\mathcal{B}$ ou $\mat(u,\mathcal{B})$ ou $\mat_\mathcal{B}(u)$.
\end{Definition}
\begin{Exemple}[Dérivée polynomial] 
Déterminer la matrice de l'endomorphisme $\Fonction{\Phi}{\mathbb {R}_n[X] }{\mathbb {R}_n[X]}{P}{P'}$ dans la base canonique de $\R_n[X]$.
\begin{Correction}
Comme $\Phi _3(X^i)=i X^{i-1}$, on a :
 $$ \uMat{\Phi}{\mathcal{B}}= \begin{pmatrix} 0 &1&0&\ldots &0 \\
 0& 0&2&\ddots \\
 \vdots &  &\ddots & \ddots \\
  \vdots &  & & \ddots& n\\
 0 &  & \dots& & 0
\end{pmatrix}. $$
\end{Correction}
\end{Exemple}

\subsection{Lien entre les opérations} 
\begin{Theoreme}[Image d'une application linéaire et image d'une matrice]
Soit $\mathcal{B}$ une base de $E$ et $\mathcal{B}'$ une base de $F$.\\
Soit $u:E\to F$ une application linéaire et $\Vect{x}$ un vecteur de $E$.\\
Alors 
$$ \uMat{u(\Vect{x})}{\mathcal{B}'} =\uMat{u}{\mathcal{B}}{\mathcal{B}'} \times \uMat{\Vect{x}}{\mathcal{B}}. $$
ou si on pose $Y= \uMat{u(\Vect{x})}{\mathcal{B'}},\quad A =\uMat{u(\Vect{x})}{\mathcal{B}}{\mathcal{B'}}$ et  $X=\uMat{\Vect{x}}{\mathcal{B}}$, on a :
$$Y=AX.$$
\end{Theoreme}
\begin{Demonstration}
Soit $\mathcal{B}=(\Vect{e_1},\dots,\Vect{e_p})$ une base de $E$ et $\mathcal{B}'=(\Vect{f_1},\dots,\Vect{f_n})$de $F$.\\
Soit $(a_{ij})_{\substack{1\leq i\leq n\\1\leq j\leq p}}$ les coefficients de la matrice $\uMat{u}{\mathcal{B}}{\mathcal{B}'}$, c'est à dire $u(\Vect{e_j})=\sum_{i=1}^n a_{ij}.\Vect{f_i}$, pour tout $j\in\Intf{1}{p}$.\\
Soit $\Vect{x}=x_1\Vect{e_1}+\dots+x_p\Vect{e_p}\in E$  et \\
On a :
$$u(\Vect{x})\overbrace{=}^{\text{linéarité}} \sum_{j=1}^p x_j u(\Vect{e_j})=\sum_{j=1}^p x_j\left(\sum_{i=1}^n a_{ij} \Vect{f_i}\right)=\sum_{i=1}^n \left(\sum_{j=1}^p x_j a_{ij}\right) \Vect{f_i}, $$
d'où $ \uMat{u(\Vect{x})}{\mathcal{B'}}=\begin{pmatrix}\sum_{j=1}^p x_j a_{1j} \\\vdots\\\sum_{j=1}^p x_j a_{nj} \end{pmatrix}$.\\
Aussi on a :

$$\begin{aligned}\\
 &\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix} \\
\uMat{u}{\mathcal{B}}{\mathcal{B'}} \times \uMat{\vec{x}}{\mathcal{B}}=\begin{pmatrix}
    a_{11} &  \dots & a_{1p}  \\
    \vdots &   &  \vdots  \\
    a_{n1} &  \dots & a_{np}
  \end{pmatrix}& \\
\uMat{u}{\mathcal{B}}{\mathcal{B'}} \times \uMat{\vec{x}}{\mathcal{B}}=\begin{pmatrix}\sum_{j=1}^p x_j a_{1j} \\\vdots\\\sum_{j=1}^p x_j a_{nj} \end{pmatrix}&.
\end{aligned}$$
Finalement on obtient bien $  \uMat{u(\Vect{x})}{\mathcal{B'}}= \uMat{u}{\mathcal{B}}{\mathcal{B'}} \times \uMat{\vec{x}}{\mathcal{B}}. $
\end{Demonstration}

\begin{Exemple}[Dérivée polynomial] 
Soit $P=1-X^2$. On a $\uMat{P}{\mathcal{B}}=\begin{pmatrix}1\\0\\-1\\0\\\vdots \end{pmatrix}$. D'où 
$$[\Phi(P)]_\mathcal{B}=  \uMat{\Phi}{\mathcal{B}} \uMat{P}{\mathcal{B}} =\begin{pmatrix} 0 &1&0&\ldots &0 \\
 0& 0&2&\ddots \\
 \vdots &  &\ddots & \ddots \\
  \vdots &  & & \ddots& n\\
 0 &  & \dots& & 0
\end{pmatrix}  \begin{pmatrix}1\\0\\-1\\0\\\vdots \end{pmatrix} = \begin{pmatrix}0\\-2\\0\\0\\\vdots \end{pmatrix}    $$
D'où le résultat escompté $\Phi(P)=P'=-2X$.
\end{Exemple}

\begin{Theoreme}[Produit matriciel et composition d'applications linéaires]
Soit $E$, $F$, $G$ trois espaces vectoriels de dimension finie munis des bases respectives $\mathcal{B}$, $\mathcal{B}'$ et $\mathcal{B}''$.\\
Soit $u:E\to F$ et $v:F\to G$ deux applications linéaires.\\
Alors
$$  \uMat{v\circ u}{\mathcal{B}}{\mathcal{B}''} = \uMat{v}{\mathcal{B}'}{\mathcal{B}''} \times \uMat{u}{\mathcal{B}}{\mathcal{B}'}. $$
\end{Theoreme}

\begin{Demonstration}
Soit $\vec{x}\in E$. On a:
$$  \uMat{v\circ u}{\mathcal{B}}{\mathcal{B}''}\uMat{\vec{x}}{B}=\uMat{v\circ u(\vec{x})}{\mathcal{B}''} $$
et
$$\uMat{v\circ u(\vec{x})}{\mathcal{B}''} =\uMat{v( u(\vec{x}))}{\mathcal{B}''}=\uMat{v}{\mathcal{B}'}{\mathcal{B}''}\uMat{u(\vec{x})}{\mathcal{B}'}= \uMat{v}{\mathcal{B}'}{\mathcal{B}''} \times \uMat{u}{\mathcal{B}}^{\mathcal{B}'}\uMat{\vec{x}}{\mathcal{B}} $$
Ainsi les matrices $\uMat{v\circ u}{\mathcal{B}}{\mathcal{B}''}$ et $\uMat{v}{\mathcal{B}'}{\mathcal{B}''} \times \uMat{u}{\mathcal{B}}{\mathcal{B}'}$ sont égales. 
\end{Demonstration}
\begin{Definition}[Application linéaire d'une matrice]
Inversement, si l'on se donne une matrice $M \in   \M{n}{q}{\K} $,
on peut définir une application linéaire, dite \defi{canoniquement associée à $M$}, par
\[ \Fonction{u_M}{\M{p}{1}{\K}}{\M{n}{1}{\K}}{X}{M\times X}.\]
Si l'on note $\mathcal{B}$ la base canonique de $\M{p}{1}{\K}$ et $\mathcal{B}'$ la base canonique de $\M{n}{1}{\K}$,
on vérifie  que
\[ [u_M]_{\mathcal{B}}^{\mathcal{B}'} = M. \]
\end{Definition}

\begin{Exemple}[Matrice de rotation]
La matrice de rotation $\begin{pmatrix}\cos \theta & -\sin \theta\\ \sin \theta & \cos \theta\end{pmatrix}$ est canoniquement associée à l'application linéaire rotation $$\Fonction{R_\theta}{\mathbb {R}^2 }{\mathbb {R}^2}{(x,y)}{ (x\cos \theta - y \sin \theta,x\sin \theta + y \cos \theta)}.$$ 
\end{Exemple}



En résumé, soit  $\mathcal{B}=(\Vect{e}_1, \dots  ,\Vect{e}_p)$ et  $\mathcal{B}'=(\Vect{f}_1, \dots  ,\Vect{f}_n)$ bases de $E$ et $F$ respectivement, nous avons :\\
\begin{center}
\begin{tabular}{c|c|c}
Vectoriel &  &Matriciel \\
\hline\hline
$\Vect{x}=x_1.\Vect{e}_1+ \dots  +x_p.\Vect{e}_p \in E$& $\longrightarrow$ & $X=[\Vect{x}]_{\mathcal{B}}=\begin{pmatrix}x_1\\ \vdots\\x_p\end{pmatrix}\in\K^p $ \\\hline
$u\in\mathcal{L}(E,F)$ &$\longrightarrow$    & \begin{tikzpicture}[
      every left delimiter/.style={xshift=0.75em},
      every right delimiter/.style={xshift=-0.75em},
      dots/.style={
        line width=1pt,
        line cap=round,
        dash pattern=on 0pt off 5pt,
        shorten >=.1cm,
    shorten <=.1cm}]
    \matrix (M) [
      matrix of nodes,
      left delimiter=(,
      right delimiter=),
    ]{
      \node (A) {$a_{11}$}; &[1.1cm] \node (B) {$a_{1n}$}; \\[1.1cm]
      \node (C) {$a_{n1}$}; &        \node (D) {$a_{nn}$}; \\
    };

    \draw (M.west) node[left] {$A=[u]_\mathcal{B}=$};
    \draw [dots] (A.east)  -- (B.west);
    \draw [dots] (C.east)  -- (D.west);
    \draw [dots] (A.south) -- (C.north);
    \draw [dots] (B.south) -- (D.north);
    \draw (A) [yshift=0.7cm] node (E) {$u(\Vect{e_1})$};
    \draw (B) [yshift=0.7cm] node (F) {$u(\Vect{e_p})$};
    \draw (B) [xshift=1cm]   node (G) {$\Vect{f_1}$};
    \draw (D) [xshift=1cm]   node (H) {$\Vect{f_n}$};
    \draw [dots] (E.east)  -- (F.west);
    \draw [dots] (G.south) -- (H.north);
  \end{tikzpicture}\\\hline
$\Vect{y}=y_1.\Vect{f}_1+ \dots  +y_n.\Vect{f}_n=u(\Vect{x})$&$\longrightarrow$ & $Y=[\Vect{y}]_{\mathcal{B}'}=\begin{pmatrix}y_1\\ \vdots\\y_n\end{pmatrix}=A\times X $ \\\hline
 $u\circ v $ avec $u,v \in\LE$ &$\longrightarrow$ &  $[u\circ v]_{\mathcal{B}} =A\times B$ avec $A=[u]_\mathcal{B}$ et $B=[v]_\mathcal{B}$\\
%$\ker u = \{\Vect{x}\in E:u(\Vect{x}) = \Vect{0}\} $      &$\longrightarrow$ &$\ker [u]_\mathcal{B}= \{X:[u]_\mathcal{B} X = 0\}$ \\\hline
%$\im u = \{u(\Vect{x}):\Vect{x}\in E\} $      & $\longrightarrow$&$\im [u]_\mathcal{B}= \{[u]_\mathcal{B} X:X\in \M{M}{n,1}{\K}\}$\\\hline
\hline

$\Fonction{u_M}{ \K^p}{ \K^n}{X}{M\times X}   $ &$\longleftarrow $ & $M\in \M{n}{p}{\K}$
 
\end{tabular}
 \end{center}
\section{Dualité entre la représentation matricielle et l'application linéaire}
\begin{Theoreme}[Caractérisation par l'image d'une base]
Pour connaître/définir une application linéaire complètement, il suffit de connaître/définir les  images d'une base de l'espace vectoriel de départ. 
\end{Theoreme}
\begin{Remarque}
Pour déterminer une application quelconque, on n'a pas d'autre choix que de déterminer les images de chaque point $x\mapsto f(x)$. En revanche,   une application linéaire est complètement déterminer par l'image d'une base. Par exemple, soit $u:(x,y)\mapsto u(x,y)$ une application linéaire avec les images de base canonique : $u(1,0)=(1,1)$ et $u(0,1)=(1,-1)$. On a 
$$u(x,y)=u(x(1,0)+y(0,1))\overbrace{=}^{\text{linéarité}}x.u(1,0)+ y.u(0,1)=x.(1,1)+ y.u(1,-1)=(x+y,x-y).$$    
\end{Remarque}
\begin{Demonstration}
Soit $\mathcal{B}=(\Vect{e_1},\dots,\Vect{e_p})$ une base de $E$.\\
Comme $u$ est linéaire, on a :
\begin{align}
u(\Vect{x})&=u(\lambda_1.\Vect{e_1}+\dots +\lambda_p.\Vect{e_p})\\
u(\Vect{x})&=\lambda_1.u(\Vect{e_1})+\dots +\lambda_p.u(\Vect{e_p})
\end{align}
Cela implique que l'application linéaire $u$ est entièrement déterminée par les vecteurs $(u(\Vect{e_1}),\dots,u(\Vect{e_p}))$.
\end{Demonstration}
\begin{Theoreme}[Dualité]
Soit $\mathcal{B}$ une base de $E$ et $\mathcal{B}'$ une base de $F$.\\
L'application
\[ \Fonction{\Phi}{\mathcal{L}(E,F)}{\M{n}{q}{\K} }{u}{[u]_\mathcal{B}^{\mathcal{B}'}} \]
est  un isomorphisme.
\end{Theoreme}
\begin{Demonstration}
On ajoute une seconde partie à la démonstration sur la caractérisation d'une application linéaire par l'image d'une base.\\
Soit $\mathcal{B}'=(\Vect{f_1},\dots,\Vect{f_n})$ une base de $F$.\\
Il existe une famille $(a_{ij})_{\substack{1\leq i\leq n\\1\leq j\leq p}}$ d'éléments de $\K$ tel que :
$$\forall j\in \Intf{1}{p}: u(\Vect{e_j})=a_{1j}\Vect{f_1}+\dots+a_{nj}\Vect{f_n}.$$ 
Ainsi, les coefficient $(a_{ij})_{\substack{1\leq i\leq n\\1\leq j\leq p}}$ de la matrice $[u]_\mathcal{B}^{\mathcal{B}'}$   caractérise entièrement l'application linéaire $u$ d'où l'isomorphisme. 
\end{Demonstration}

\begin{Corollaire}[Dimension de $\mathcal{L}(E,F)$]
$$\dim \mathcal{L}(E,F) = \dim(E) \times\dim(F).$$
\end{Corollaire}
\begin{Demonstration}
Comme l'application $\Phi$ est un isomorphisme, la dimension de l'espace vectoriel de départ est égale à celle d'arrivé (voir la section image et noyau). Donc on a $$\dim \mathcal{L}(E,F)=\dim \M{n}{q}{\K}= n q= \dim (F)\dim (E).$$ 
\end{Demonstration}

\begin{Proposition}[Inversibilité à gauche et à droite d'une matrice carré]
Soit $A,B\in \MnK$.
Si $AB=I_n$ alors $A$ et $B$ sont toutes deux inversibles et inverses l'une de l'autre.
\end{Proposition}
\begin{Demonstration}
Soit $A,B\in \MnK$ tel que $AB=I_n$. D'où $u_{AB}=u_{I_n}$.  Comme $u_{AB}(X)=ABX=A u_{B}(X) =u_{A}( u_{B}(X))$, on a $u_{AB}=u_{A}\circ u_{B}$. Ainsi on obtient $u_{A}\circ u_{B}=u_{I_n}$ donc que  $u_{A}$ est inversible à droite d'inverse $u_{B}$. Du fait de la dimension finie,  $u_{A}$ est inversible d'inverse $u_{B}$. Donc $A$ et $B$ sont toutes deux inversibles et inverses l'une de l'autre.
\end{Demonstration}

\begin{Proposition}[Dualité de l'inversibilité]
Soit $u$ un endomorphisme. 
$$u\in\GLE\Leftrightarrow [u]_\mathcal{B}\in \GLnK.$$
Soit $M$ une matrice carré.
$$M \in \GLnK \Leftrightarrow u_M\in \GLE[\K^n] .$$
\end{Proposition}



\begin{Demonstration}
Soit $u\in\GLE$. Donc il existe $v\in\GLE$ tel que $u\circ v=Id_E$ et $v\circ v =Id_E$. D'où    

Comme l'application $\Phi$ est un isomorphisme d'anneau, 
\end{Demonstration}



\begin{Definition}[Rang] Le \defi{rang} d'une  application linéaire $u$ est  la dimension de l'espace vectoriel $\Ima u$.\\
Le \defi{rang} d'une matrice est le rang de l'application linéaire canoniquement associée.
\end{Definition}
\begin{Proposition} Le rang d'une matrice est le rang de la famille de ses vecteurs colonnes. 
\end{Proposition}
\begin{Proposition}[Dualité]
Soit $E$ un espace vectoriel de dimension n. Soit $\mathcal{B}$ une base de $E$. \\
$u$ est injective si et seulement si $\Ker [u]_\mathcal{B}=\{0\}$.
\end{Proposition}

\begin{Definition}[Noyau et image]
\begin{itemize}
\item
  L'\defi{image} de $M$ est le sous espace vectoriel $u_M(\M{n}{1}{\K}) = \{MX:X\in \M{n}{1}{\K}\}$;
  on le note $\im M$.
\item
  Le \defi{noyau} de $M$ est le le sous espace vectoriel $u_M^{-1}\{ \{0 \}\} = \{X\in  \M{n}{1}{\K}:MX=0\}$;
  on le note $\Ker M$.
\end{itemize}
\end{Definition}

\begin{Exemple}
Soit $M=\begin{pmatrix}
1&1&1\\
1&-1&0\\
\end{pmatrix}$
On a $M\begin{pmatrix}x \\y \\z\\\end{pmatrix}=x\begin{pmatrix}1\\1\\ \end{pmatrix}+y\begin{pmatrix}1\\-1\\ \end{pmatrix}+z\begin{pmatrix}0\\1\\ \end{pmatrix}$.
Donc $\Ima M$ est l'espace vectoriel engendré par la famille $(\begin{pmatrix}1\\1\\ \end{pmatrix},\begin{pmatrix}1\\-1\\ \end{pmatrix},\begin{pmatrix}0\\1\\ \end{pmatrix})$ d'où $\Ima M=\M{2}{1}{\K}$.
$$\begin{pmatrix}x \\y \\z\\\end{pmatrix} \in \Ker M \Leftrightarrow M\begin{pmatrix}x \\y \\z\\\end{pmatrix}=\begin{pmatrix}0 \\0 \\0\\\end{pmatrix} \Leftrightarrow \begin{cases}x+y+z=&0\\x-y=&0\end{cases}\overbrace{\Leftrightarrow}^{x=t} \exists t \in \R: \begin{pmatrix}x \\y \\z\\\end{pmatrix}=t\begin{pmatrix}1 \\1 \\-2\\\end{pmatrix} .$$
Donc $\Ker M =\R \begin{pmatrix}1 \\1 \\-2\\\end{pmatrix}.$\\
Comme $M$ est la matrice associée à l'application linéaire de l'exemple~\ref{ex:noyau}, on retrouve les mêmes résultats en identifiant les n-uplets aux matrices colonnes. 
\end{Exemple}
\begin{Proposition}[Image d'une base]
L'image d'une matrice est l'espace vectoriel engendré par ses colonnes.
\end{Proposition}
\begin{Theoreme}[Théorème du rang version matricielle]

Soit $M \in   \M{n}{q}{\K} $.

Alors 
\[ p = \Rang M + \dim \Ker M. \]
\end{Theoreme}
%% --------------
\subsection{Changement de base}
Il faut bien garder à l'esprit que la matrice d'une application linéaire est une "représentation"
de celle-ci qui dépend du choix des bases au départ et à l'arrivée. Il est utile de savoir passer
d'une représentation à une autre. Connaissant la matrice d'une application linéaire dans deux
bases, il faut savoir la déterminer dans deux autres bases.
\subsection{D'une représentation à l'autre}
\begin{Definition}[Matrice de passage]
Soit $E$ un espace vectoriel de dimension finie et  $\mathcal{B}$ et $\mathcal{B}'$ deux bases de $E$.
On appelle \defi{matrice de passage} de $\mathcal{B}$ à $\mathcal{B}'$ la matrice
\[ P_{\mathcal{B}\to\mathcal{B}'} = [\mathrm{Id}_E]_{\mathcal{B}'}^{\mathcal{B}}. \]

De façon plus explicite, notons $\mathcal{B} = \{\Vect{e_1},\dots,\Vect{e_n}\}$,
$\mathcal{B}' = \{\Vect{f_1},\dots,\Vect{f_n}\}$ et $P_{\mathcal{B}\to\mathcal{B}'} = ( a_{ij} )_{1\leq i \leq n,1\leq j \leq n}$.
Dans ce cas, on a
\[ \forall   j \in   \{1,\dots,n\}:\quad \Vect{f_j} = \sum  _{i=1}^n a_{ij} \Vect{e_i}. \]
Ainsi,
\begin{center}
  \begin{tikzpicture}[
      every left delimiter/.style={xshift=0.75em},
      every right delimiter/.style={xshift=-0.75em},
      dots/.style={
        line width=1pt,
        line cap=round,
        dash pattern=on 0pt off 5pt,
        shorten >=.1cm,
    shorten <=.1cm}]
    \matrix (M) [
      matrix of nodes,
      left delimiter=(,
      right delimiter=),
    ]{
      \node (A) {$a_{11}$}; &[1.1cm] \node (B) {$a_{1n}$}; \\[1.1cm]
      \node (C) {$a_{n1}$}; &        \node (D) {$a_{nn}$}; \\
    };

    \draw (M.west) node[left] {$P_{\mathcal{B}\to\mathcal{B}'}=$};
    \draw [dots] (A.east)  -- (B.west);
    \draw [dots] (C.east)  -- (D.west);
    \draw [dots] (A.south) -- (C.north);
    \draw [dots] (B.south) -- (D.north);
    \draw (A) [yshift=0.7cm] node (E) {$\Vect{f_1}$};
    \draw (B) [yshift=0.7cm] node (F) {$\Vect{f_n}$};
    \draw (B) [xshift=1cm]   node (G) {$\Vect{e_1}$};
    \draw (D) [xshift=1cm]   node (H) {$\Vect{e_n}$};
    \draw [dots] (E.east)  -- (F.west);
    \draw [dots] (G.south) -- (H.north);
  \end{tikzpicture}
\end{center}
\end{Definition}
\begin{Proposition}[Inversible]
Si $P$ est une matrice de passage alors $P$ est inversible.\\
Si  $P$ est inversible alors $P$ est la matrice de passage de la base canonique à la base de ses colonnes $(C_1,\dots, C_n)$.
\end{Proposition}


 \begin{Exemple}
Dans $\R ^2$, si $\mathcal{B}$ est la base canonique et si $\mathcal{B}' = \left( \begin{pmatrix}
1\\2
\end{pmatrix},\begin{pmatrix}
-1\\3
\end{pmatrix}\right) $, alors :\\
$P_{\mathcal{B}\to\mathcal{B}'}=\begin{pmatrix}
1&-1\\2&3
\end{pmatrix}$et $P_{\mathcal{B}'\to\mathcal{B}}=\begin{pmatrix}
\frac 35&\frac 15\\-\frac 25&\frac 15
\end{pmatrix}$
La première matrice de passage ne nécessite aucun calcul, la seconde résulte de la méthode du Pivot de Gauss:
$$\begin{array}{rl}
&\left({\begin{array}{rr|rr}1 &-1& 1 & 0\\ 2 & 3 &0&1 \end{array}}\right)\\
\begin{small}\begin{array}{r} \\(L2)\leftarrow (L2)-2(L1) \end{array}\end{small}&\left({\begin{array}{rr|rr}1 &-1& 1 & 0\\ 0 & 5 &-2&1 \end{array}}\right)\\
\begin{small}\begin{array}{r} \\(L2)\leftarrow \frac 1 5 (L2) \end{array}\end{small}&\left({\begin{array}{rr|rr}1 &-1& 1 & 0\\ 0 & 1 &- \frac 2 5& \frac 1 5 \end{array}}\right)\\
\begin{small}\begin{array}{r}(L1)\leftarrow (L1)+(L2)  \\ \end{array}\end{small}&\left({\begin{array}{rr|rr}1 &0&  \frac 3 5  &  \frac 1 5\\ 0 & 1 &- \frac 2 5& \frac 1 5 \end{array}}\right)
\end{array}.$$
 \end{Exemple}
  \begin{Exemple}
Dans $\R _2[X]$, le $\R $-espace vectoriel constitué des polynômes à coefficients réels de degré $\leq 2$, considérons la base $\mathcal{B}=(X^i)_{0\leq i \leq 2}$ et  $\mathcal{B}'=((X-1)^i)_{0\leq i \leq 2}$.
\[\text{Comme }
\left \{
\begin{array}{c @{=} l}
   1 & 1\times 1 \\
  (X-1) & (-1)\times 1 + 1\times X \\
  (X-1)^2 & 1\times 1 + (-2)\times X + 1\times X^2
\end{array}
\right. , \text{ on a }P_{\mathcal{B}\to\mathcal{B}'}=\begin{pmatrix}
1&-1&1\\0&1&-2\\0&0&1
\end{pmatrix}.
\]
 \end{Exemple}
\begin{Proposition}
Soit $E$ un espace vectoriel de dimension finie et $\mathcal{B}$, $\mathcal{B}'$ et $\mathcal{B}''$ trois bases de $E$.
Alors
\begin{enumerate}
\item
  $P_{\mathcal{B} \to \mathcal{B}'} × P_{\mathcal{B}' \to \mathcal{B}''} = P_{\mathcal{B} \to \mathcal{B}''}$;
\item
  $P_{\mathcal{B} \to \mathcal{B}'}$ est inversible, et son inverse est $P_{\mathcal{B}' \to \mathcal{B}}$.
\end{enumerate}
\end{Proposition}

\begin{Proposition}[Vecteur colonne]
Soit $E$ un espace vectoriel de dimension finie, $\mathcal{B}$ et $\mathcal{B}'$ deux bases de $E$.
Soit $\Vect{x}$ un vecteur de $E$.\\
Alors \[ X = PX'\]
où
\begin{itemize}
\item $P = P_{\mathcal{B} \to \mathcal{B}'}$,
\item $X = [\Vect{x}]_{\mathcal{B}}$,
\item $X' = [\Vect{x}]_{\mathcal{B}'}$.
\end{itemize}
\end{Proposition}
\begin{Proposition}
Soit $E$ un espace vectoriel de dimension finie muni de deux bases $\mathcal{B}$ et $\mathcal{B}'$.
Soit $F$ un espace vectoriel de dimension finie muni de deux bases $\mathcal{C}$ et $\mathcal{C}'$.
Soit $u:E\to F$ une application linéaire.\\
Alors on a \[ A' = Q^{-1} A P \]
où
\begin{itemize}
\item $P = P_{\mathcal{B} \to \mathcal{B}'}$,
\item $Q = P_{\mathcal{C} \to \mathcal{C}'}$,
\item $A = [u]_{\mathcal{B}}^{\mathcal{C}}$,
\item $A' = [u]_{\mathcal{B}'}^{\mathcal{C}'}$.
\end{itemize}
\end{Proposition}
\begin{Corollaire}

Soit $E$ un espace vectoriel de dimension finie muni de deux bases $\mathcal{B}$ et $\mathcal{B}'$.
Soit $u$ un endomorphisme de $E$.
Alors on a \[ A' = P^{-1} A P \]
où
\begin{itemize}
\item $P =  P_{\mathcal{B} \to \mathcal{B}'}$,
\item $A =  [u]_{\mathcal{B}}$,
\item $A' =  [u]_{\mathcal{B}'}$.
\end{itemize}
\end{Corollaire}

\subsection{Similitude}

\begin{Definition}[Similitude]
Soit $A$ et $B$ deux matrices carrées de $\MnK$.
On dit que $A$ et $B$ sont \defi{semblables} si et seulement si existe une matrice inversible $P\in  \GLnK$
telle que \[ B = P^{-1} A P. \]
\end{Definition}
\begin{Exemple}  Soit $M=\begin{pmatrix}0&1\\1&0\end{pmatrix}$ représentant la symétrie orthogonal par rapport à la première bissectrice. Alors $M$ est semblable à la matrice diagonale $D=\begin{pmatrix}1&0\\0&-1\end{pmatrix}$ avec $P$ la matrice de passage de la base canonique à la base $(\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\-1\end{pmatrix})$ d'où $P=\begin{pmatrix}1&1\\1&-1\end{pmatrix}$ et $P^{-1}=\frac 1 2\begin{pmatrix}1&1\\1&-1\end{pmatrix}$. On a bien $M=PDP^{-1}$.
\end{Exemple}
\begin{Proposition}[Propriétés]
Il s'agit d'une relation d'équivalence, c'est à dire:
\begin{enumerate}
\item $A \sim A$;
\item si $A \sim B$, alors $B \sim A$;
\item si $A \sim B$ et $B \sim C$, alors $A \sim C$.
\end{enumerate}
\end{Proposition}
\begin{Remarque}
Deux matrices semblables ont même déterminant, même trace, même rang, même
polynôme caractéristique, même valeurs propres. La réciproque est fausse.

\end{Remarque}

\begin{Proposition}
Soit $u$ un endomorphisme de $E$. Soit $\mathcal{B}$ et $\mathcal{B}'$ deux bases de $E$.\\
Alors les matrices $[u]_\mathcal{B}$ et $[u]_{\mathcal{B}'}$ sont semblables.
\end{Proposition}
\begin{Proposition}
Soit $A$ et $B$ deux matrices.
Soit $E$ un espace vectoriel de dimension $n$ et $\mathcal{B}$ une base de $E$.
Notons $u$ l'unique endomorphisme de $E$ tel que $[u]_\mathcal{B} = A$.
Alors $A$ et $B$ sont semblables si et seulement si il existe une base $\mathcal{B}'$ de $E$ telle que $[u]_{\mathcal{B}'} = B$.\\
C'est à dire que deux matrices semblables représentent la même application linéaire dans deux bases différentes.
\end{Proposition}


\end{document}
